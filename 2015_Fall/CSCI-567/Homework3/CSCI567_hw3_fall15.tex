%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Structured General Purpose Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%       PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{amsmath}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in 

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ : \hmwkTitle} % Top center header
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%       DOCUMENT STRUCTURE COMMANDS
%       Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}
   
%----------------------------------------------------------------------------------------
%       NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment\ \#3 } % Assignment title
\newcommand{\hmwkDueDate}{Friday,\ October\ 16,\ 2015} % Due date
\newcommand{\hmwkClass}{CSCI-567} % Course/class
\newcommand{\hmwkClassTime}{} % Class/lecture time
\newcommand{\hmwkAuthorName}{Saket Choudhary} % Your name
\newcommand{\hmwkAuthorID}{2170058637} % Teacher/lecturer
%----------------------------------------------------------------------------------------
%       TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassTime}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName} \\
        \textbf{\hmwkAuthorID}
        }
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%       TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

\newpage
\tableofcontents
\newpage


%----------------------------------------------------------------------------------------
%       PROBLEM 2
%----------------------------------------------------------------------------------------





\begin{homeworkProblem}[Problem 1] % Custom section title
\begin{homeworkSection}{\homeworkProblemName: ~(a)}

\problemAnswer{
	Let $\sigma(a) = \frac{1}{1+e^{-a}}$ and 
	
	$$ P(Y=1|X=x) = \sigma(b+w^Tx)\\
	\newline
	P(Y=0|X=x) = 1-\sigma(b+w^Tx)$$
	
	Observe that $Y=1$ when $b+w^Tx \geq 0$ and $Y=0$ when $b+w^Tx < 0$
	
	Thus,
	 
	\begin{align*}
	P(Y=y|X=x) &= \sigma(b+w^T)^y (1-\sigma(b+w^Tx))^({1-y})\\
	\log(P(Y=y|X=x)) &= y\log(\sigma(b+w^Tx))^y +  ({1-y}) \log(1-\sigma(b+w^Tx))\\
	&= y \log(\frac{\sigma(b+w^T)}{1-\sigma(b+w^Tx)}) + \log(1-\sigma(b+w^Tx))\\
	&= y(b+w^Tx) + \log(\frac{e^{-(b+w^Tx)}}{1+e^{-(b+w^Tx)}})\\
	&= y(b+w^Tx) + \log(\frac{1}{1+e^{(b+w^Tx)}})\\
	&= y(b+w^Tx) - \log(1+e^{(b+w^Tx)})\tag{[1.1]}
	\end{align*}
	
	\begin{align*}
		\mathcal{L}(\boldmath{w}) &= -\log(\prod_{i=1}^nP(Y=y_i|X=x_i))\\
		&= -\sum_{i=1}^n \log(P(Y=y_i|X=x_i))\\
		&= -\sum_{i=1}^n\big( \boldmath{y_i}(b+w^T\boldmath{x_i}) - \log(1+e^{(b+w^T\boldmath{x_i})})\big)\\
%		&= \sum_{i=1}^n\big( \boldmath{-y_i}(b+w^T\boldmath{x_i}) + \log(1+e^{(b+w^T\boldmath{x_i})})\big)\\
		%&= -\sum_{i=1}^n\big( \log(e^{(\boldmath{y_i}(b+w^T\boldmath{x_i}))}) - \log(1+e^{(b+w^T\boldmath{x_i})})\big)\\
		%&= -\sum_{i=1}^n\big( \log(e^{(\boldmath{y_i}(b+w^T\boldmath{x_i}))}) - \log(1+e^{-y_(b+w^T\boldmath{x_i})})\big)\\	
	\end{align*}
	
	
	Consider $\mathcal(L)(w) = y(b+w^Tx) - \log(1+e^{(b+w^Tx_i)})$
	
	
	\begin{align*}
	\frac{\partial \mathcal{L}(w)}{\partial w} &= -(xy^T) + \frac{e^{(b+w^Tx)}x}{1+e^{(b+w^Tx)}}\\
	\frac{\partial^2 \mathcal{L}(w)}{\partial w^2}&= 0 +  \frac{\partial }{\partial w}\big(x-\frac{x}{1+e^{(b+w^Tx)}}\big)\\
	\frac{\partial^2 \mathcal{L}(w)}{\partial w^2}&= \frac{x(e^{(b+w^Tx)})x^T}{(1+e^{(b+w^Tx)})^2} \geq 0\ \forall\ x \in \mathbf{R}\\
	\frac{\partial^2 \mathcal{L}(w)}{\partial w^2}&= x^T \sigma(b+w^Tx)(1-\sigma(b+w^Tx))x \geq 0  \tag{1.2}\\
	\end{align*}
	
	From (1.2) $\frac{\partial^2 \mathcal{L}(w)}{\partial w^2} \geq 0$ and hence, from the definition of convex functions, $\mathcal{L}(w)$ is indeed a convex function.
	}
\end{homeworkSection}
\begin{homeworkSection}{\homeworkProblemName: ~(b)}

\problemAnswer{
	
	
	When the data is perfectly linearly separable, (assume first $n/2$ of the $n$ training points belong to class 0 and the remaining to class $1$), thus our regression model should assign the first $n/2$ points to class with cent percent certainity or with probability 1 and the remaining $n/2$ to class 0 with probability 1.
	For this to happen, $P(Y=1|X=X_1) = 1$ and $P(Y=0|X=X_0)=1$ where $X_1$ is the set of points belonging to class $1$ and $X_0$ is the set of points belonging to class 0.
	
	Clearly this scenario is possible when $||w|| \longrightarrow \infty$
}
\end{homeworkSection}
\begin{homeworkSection}{\homeworkProblemName: ~(c)}
	\problemAnswer{

	A simple example with two points would be $(0,0)$, $(1,1)$. Intuitively the step function's step branches (the horizontals of a sigmooid function) will be located at infinity. Also the line separating the points (0,0) and (1,1) can be anywhere in between 0 and 1, thus there will be multiple solutions.
}
\end{homeworkSection}

\begin{homeworkSection}{\homeworkProblemName: ~(d)}
	\problemAnswer{
		\begin{align*}
		\mathcal{L}(w) &= \sum_{j=1}^n\big( \boldmath{-y_j}(b+w^T\boldmath{x_j}) + \log(1+e^{(b+w^T\boldmath{x_j})})\big) + \lambda ||w||_2^2\\
		\frac{\partial \mathcal{(L)}(w)}{\partial w_i} &=  \sum_{j=1}^n\big( \boldmath{-y_j}(\boldmath{x_{ji}}) + \frac{x_{ji}e^{(b+w^T\boldmath{x_j}) x_{ij}}}{1+e^{(b+w^T\boldmath{x_j})}}\big) + 2\lambda w_i = 0\\
		\frac{\partial^2 \mathcal{(L)}(w)}{\partial w_i^2} &=  \sum_{j=1}^n\big(  \frac{x_{ji}^2e^{(b+w^T\boldmath{x_j}) x_{ij}}}{(1+e^{(b+w^T\boldmath{x_j})})^2}\big) + 2\lambda > 0
		\end{align*}
		where the last inequality holds since $\lambda > 0$ 
		Consider $f(w_i) = \sum_{j=1}^n\big( \boldmath{-y_j}(\boldmath{x_{ji}}) + \frac{x_{ji}e^{(b+w^T\boldmath{x_j}) x_{ij}}}{1+e^{(b+w^T\boldmath{x_j})}}\big) + 2\lambda w_i = 0$
		
		And $u,v$ are the two solutions of $f(w_i)=0$, i.e. $f(u) = f(v) = 0$ (Without loss of generality, assume $u < v$)
		
		By Rolle's theorem, If $f(u)=f(v)=0$ then there exists a point in $[u,v]$ say $c$ such that $f'(c)=0$ for $c \in [u,v]$
		
		But, $f'(w_i) = \sum_{j=1}^n\big(  \frac{x_{ji}^2e^{(b+w^T\boldmath{x_j}) x_{ij}}}{(1+e^{(b+w^T\boldmath{x_j})})^2}\big) + 2\lambda > 0$
		and hence there exists no such $c$.
		
		and hence the function is convex, thus the solution to the partial differential $\frac{\partial \mathcal{(L)}(w)}{\partial w_i}$ is unique.
	}
\end{homeworkSection}
\end{homeworkProblem}

\begin{homeworkProblem}{Problem 2}
	\begin{homeworkSection}{\homeworkProblemName: ~(a)}
	\problemAnswer{
		Consider $||w||_0= \#{i: w_i \neq 0}$ for a 1D case.
		Where, $x_1=(0)$ and $x_2 = (\epsilon)$ where $0 < \epsilon << 1$
		
		$f(\boldmath{w}) = \sum_{i} I\{w_i\neq 0\}$
	
		Since we are in 1D:
		
		$f(\boldmath{w}) = \begin{cases}
		0 & \text{if w=0}\\
		1 & \text{otherwise}
		\end{cases}$
		
		Thus,
		\begin{align*}
		f(0) &= 0\\
		f(\epsilon) &= 1\\
		f(\lambda \times 0 + (1-\lambda)\times \epsilon) &= 1 \forall 0 < \lambda < 1 \tag{2a.1}\\
		\lambda f(0) + (1-\lambda)f(\epsilon) &= 1-\lambda 
		0 < 1-\lambda &< 1 \tag{2a.2}
		\end{align*}
		
		From $(2a.1),(2a.2)$ we see:
		
		$f(\lambda \times 0 + (1-\lambda)\times \epsilon) > \lambda f(0) + (1-\lambda)f(\epsilon)$ 
	
		 
		
		Thus, $||w||_0$ is not a convecx function!
		
	
		
		
	}
	\end{homeworkSection}
	
	\begin{homeworkSection}{\homeworkProblemName: ~(b)}
	\problemAnswer{
		$||w||_1 = \sum_i |w_i|$
		
		Consider two vectors $u,v$(same dimension say in $\mathbf{R^D}$)
		
		Assume: $0 \lambda < 1$
		
		\begin{align*}
		||\lambda u + (1-\lambda) v|| & = \sum_{i=1}^D |\lambda u_i + (1-\lambda) v_i|\\
		&\leq \sum_{i=1}^D \big(|\lambda u_i| + |(1-\lambda) v|\big)\  (since\ |a+b| \leq |a| + |b| \forall\ a,b \in R )\\
		&= \sum_{i=1}^D |\lambda| |u_i| + \sum_{i=1}^D |1-\lambda| |v_i|\\
		&= \lambda ||u||_1 + (1-\lambda)||v||_1\ \text{since} (0 \lambda < 1) \tag{2a.1}
		\end{align*}
		
		From $(2b.1)$, we see.
		$||\lambda u + (1-\lambda) v||_1 \leq \lambda ||u||_1 + (1-\lambda)||v||_1$
		
		And hence, $||w||_1$ is a convex function.
		}
	\end{homeworkSection}
	
	\begin{homeworkSection}{\homeworkProblemName: ~(c)}
		\problemAnswer{
			
			%The idea of regularisation here is to bound the hyperparameters $w_i$ so that they are less prone to overfitting. One simple QP equaivalent is this:
			
			%$min_{w}\frac{1}{2}(y_1-w^Tx_1, y_2-w^Tx_2 \cdots y_n-w^Tx_n )^T I (y_1-w^Tx_1, y_2-w^Tx_2 \cdots y_n-w^Tx_n )$ where $I$ is the $n \times n$ identity matrix and
			%the constraint is:
			
			Let's redefine(for the sake of easense)$x_i$ to be column vector i.e $x_i \ is D\times1$ $w^T is 1\times D$ and and $Y=(y_1 \dots y_n)$ the equivalent porblem then becomes:
			$$min_w \sum_i(y_i-x_i^w)^2$$
			%$||w||_1 \leq \frac{1}{\lambda}$
$$
			\min_w \sum_i(y_i-x_i^Tw)^2 + \lambda ||w||_1$$
		$$	\min_w (y-X^Tw)^T(y-X^Tw) + \lambda ||w||_1$$
		$$	\min_w (w^TXX^Tw-2Y^TXw+Y^TY) +  \min_w \lambda ||w||_1$$
		$$	\min_w (w^TXX^Tw-2Y^TXw) +  \min_w \lambda ||w||_1$$
We introduce dummy variables $t_i$ such that:

$$||w_i|| \leq t_i \implies t_i \geq w_i \text{and} t_i \geq -w_i$$

Now,
$$
\min_w \lambda ||w||_1 \leq \lambda (t_1+t_2+\dots + t_n)
$$
Constraint:
\begin{align*}
t_i + w_i&\geq \\
t_i - w_i &\geq -w_i
\end{align*}

which in the matrix form looks like:

$$
\begin{pmatrix}
1 & 1\\
1 & -1
\end{pmatrix} \begin{pmatrix}
t_i\\
w_i
\end{pmatrix} \geq 0
$$

Now consider this vector,: $$\begin{pmatrix}
t_1\\
t_2\\
t_3\\
\vdots
t_n\\
w_1\\
\vdots
w_n
\end{pmatrix}$$		

The matrix $A$ for reducing this constraicnt to the form $Au < b$ is then given by:
Let:
$$
B= \begin{pmatrix}
1 & 1\\
1 & -1
\end{pmatrix}
$$

$$
\begin{pmatrix}
1 & (n-1)zeroes\dots         & 1 & 0\dots & \\
1 & (n-1)zeroes\dots         & -1 & 0\dots & \\
01 & 1 &(n-1)zeroes       & -1 & \\
&         & &  -1
\end{pmatrix}\begin{pmatrix}
t_1\\
t_2\\
t_3\\
\vdots
t_n\\
w_1\\
\vdots
w_n
\end{pmatrix} \geq 0
$$
}
\newpage
\problemAnswer{
Our optimisation problem now looks like:


$$
\begin{pmatrix}
t\\
w
\end{pmatrix}^T\begin{pmatrix}
0 & 0\\
0 & XX^T
\end{pmatrix}
\begin{pmatrix}
t \\
w
\end{pmatrix}
+ \begin{pmatrix}
	1 & 1 &  \dots(d-2) \text{times } 1 & \text{d times } 0\dots
\end{pmatrix}\begin{pmatrix}
t\\
w
\end{pmatrix}
$$
			}
	\end{homeworkSection}

\end{homeworkProblem}

\begin{homeworkProblem}[Problem 3]
\begin{homeworkSection}{\homeworkProblemName: ~(a)}
	\problemAnswer{
		
		$min_w(\sum_i (y_i-w^Tx_i)^2+ \lambda||w||_2^2)$
		
		In more compact matrix notation, let:
		
		
		\begin{align*}
		y_{n\times 1} &= (y_1\ y_2\ \cdots\ y_n)^T\\
		X_{n \times D} &= (x_1^T \ x_2^T\ \cdots\ x_n^T)^T\\
		\end{align*}
		
		This notation, reduces the above function to:
		
		$min_w(||y-w^TX||_2^2 + \lambda||w||_2^2)$
		
		\begin{align*}
		f(w) &= min_w(||y-Xw||_2^2 + \lambda||w||_2^2)\\
		&= (y-Xw)^T(y-Xw) + \lambda w^Tw\\
		&= (y^T-w^TX^T)(y-Xw) + \lambda w^Tw\\
		&= y^Ty - y^TXw -w^TX^Ty + w^TX^TXw + \lambda w^Tw\\
		&= y^Ty - {(X^Ty)}^Tw -w^TX^Ty + w^TX^TXw	 + \lambda w^Tw\\
		\frac{\partial f(w)}{\partial w} &= -X^Ty - X^Ty + 2\lambda w + (X^TXw + (XX^Tw)) = 0\\
		&= 2\lambda w +2X^TXw -2X^Ty = 0\\
		\textbf{w}(\lambda I_D + X^Tw) &= X^Ty\\
		\textbf{w} &= (X^TXw + \lambda I_D)^{-1}X^Ty
		\end{align*}
		
		
		}
\end{homeworkSection}

\begin{homeworkSection}{\homeworkProblemName: ~(b)}
	\problemAnswer{
		$min_w(||y-w^T\Phi||_2^2 + \lambda||w||_2^2)$
		From the previous part, the solution should be of similar form:
		
		$\textbf{{w}} = (\Phi^T\Phi + \lambda I_D)^{-1}\Phi^Ty$
		
		Using the identity:
		
		$(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1}$
		
		Thus,
		
		$\big((\lambda I_D + \Phi^T\Phi)^{-1}\big)\Phi^Ty =  \Phi^T\big(\Phi\Phi^T + \lambda I_N\big)^{-1}y$
		
		$w^{*} = \Phi^T(\Phi\Phi^T + \lambda I_N)^{-1} y$
		
	}
	
\end{homeworkSection}

\begin{homeworkSection}{\homeworkProblemName: ~(c)}
	\problemAnswer{
		$\hat{y} = w^{*T} \Phi(x)$
		
		$\hat{y} =  \big(\Phi^T(\Phi\Phi^T + \lambda I_N)^{-1} y\big)^T\Phi(x) = y^T \big((\Phi\Phi^T + \lambda I_N)^{-1}\big)^T\Phi^T\Phi(x)$
		
		Now using the property, $(A^{-1})^T = (A^T)^{-1}$
		
		
		\begin{align*}
		\hat{y} &=  = y^T \big((\Phi\Phi^T + \lambda I_N)^{-1}\big)^T\Phi^T\Phi(x) \\
		&=  y^T \big((\Phi\Phi^T + \lambda I_N)^{T}\big)^{-1}\Phi^T\Phi(x)\\
		&=  y^T \big((\Phi^T\Phi + \lambda I_N)\big)^{-1}\Phi^T\Phi(x)\\
		&= y^T(K+\lambda I_N)^{-1} \kappa(x)
		\end{align*}
		
		Where $K_{ij}= \Phi_i^T\Phi_j$ and $\kappa(x) = \phi^T\phi^T(x)$
		
		
		
		}
\end{homeworkSection}
\begin{homeworkSection}{\homeworkProblemName: ~(d)}
\problemAnswer{
	
	Kernel ridge regression is $O(n^3)$ for $n$ data points. Linear regression was forumlated as quadratic programing and hence is $O(n^2)$.
	
	The extra $n$ factor comes from the formation of kernel matrix $K$.
	}
\end{homeworkSection}

\end{homeworkProblem}

\begin{homeworkProblem}[Problem 4] % Custom section title
	
	Given: $k_1(.,.)$ and $k_2(.,.)$ are kernel function. Thus,
	for any vector $y \in \mathbf{R}$,  $y^TKy \geq 0$ where $K_{ij} = k(x_i, x_j)$
	
	Mercer's theorem requires $K$ to be positive semi-definite.
	
	\begin{homeworkSection}{\homeworkProblemName: ~(a)} % Section within problem
		\problemAnswer{
			$k_3(x,x') = a_1k_1(x,x')+a_2k_2(x,x')\ \text{where} a_1,a_2\geq 0$
			
			Since $k_1(x,x')$ is positive definite, $\forall y \in \mathbf{R}$, 
			
			\begin{align*}
			y^TK^{(1)}y \geq 0 \tag{4a.1},\\
			\text{where}\\
			K^{(1)}_{ij} = k_1(x_i,x_j')
			\end{align*}
			
			Similarly,
			
			
			\begin{align*}
			y^TK^{(2)}y \geq 0 \tag{4a.2},\\
			\text{where}\\
			K^{(2)}_{ij} = k_2(x_i,x_j')
			\end{align*}
			
			Thus, from (4a.1) and (4a.2), we get
			
			\begin{align*}
			y^T(K^{(1)}+K^{(2)})y \geq 0\  \forall y \in \mathbf{R}
			\implies\\
			y^TK^{(3)}y \geq 0\  \forall y \in \mathbf{R}\\
			\text{where}\\
			K^{(3)}_{ij} = k_3(x_i,x_j')
			\end{align*}
			
			
		}
	\end{homeworkSection}
	
	
	
	\begin{homeworkSection}{\homeworkProblemName: ~(b)} % Section within problem
		\problemAnswer{ % Answer
			$k_4(x,x') =f(x)f(x')$ 
			Let $K^{(4)}_{ij} = k_4(x_i,x_j) = f(x_i)f(x_j')$
			
			Since $f(x)$ is a real valued function, consider $K^{(4)}$
			\begin{align*}
			K^{(4)} = \begin{bmatrix}
			f(x_1)f(x_1') & f(x_1)f(x_2') & \cdots & f(x_1)f(x_n')\\
			\vdots\\
			f(x_n)f(x_1') & f(x_n)f(x_2') & \cdots & f(x_n)f(x_n')
			\end{bmatrix}\\
			K^{(4)} = \vec{F({x})}_{n\times 1} \vec{F(x)}^T_{1 \times n} \\
			\text{where}\\
			F(x)^T_{1 \times n} = \begin{pmatrix}
			f(x_1)\\
			f(x_2)\\
			\vdots
			f(x_n)
			\end{pmatrix}
			\end{align*}
			
			Now consider $y^TK^{(4)}y = y^TF(x)F(x)^Ty = y^TF(x)(y^TF(x))^T = ||y^TF(x)||_2^2 \geq 0$
			
			Thus, $k_2(.,.)$ is a valid kernel function!.
		}
		
	\end{homeworkSection}
	\begin{homeworkSection}{\homeworkProblemName: ~(c)}
		\problemAnswer{
			$k_5(x,x') = g(k_1(x,x'))$ where $g$ is a polynomial with positive coefficients.
			
			Since $g$ has positive coefficients,
			$g(x) \geq 0 \forall x \geq 0$
			
			Now consider,
			\begin{align*}
			y^TK^{(5)}y  &= (y_1\ y_2 \cdots y_n) \times \begin{bmatrix}
			g(k_1(x_1,x_1')) & g(k_1(x_1,x_2')) & \cdots g(k_1(x_1,x_n'))\\
			\vdots\\
			g(k_1(x_n,x_1')) & g(k_1(x_n,x_2')) & \cdots g(k_1(x_n,x_n'))
			\end{bmatrix} \times \begin{pmatrix}
			y_1\\
			y_2\\
			\vdots\\
			y_n
			\end{pmatrix}\\
			y^TK^{(5)}y  &= y_1g(k_1(x_1,x_1'))y_1 + y_2g(k_1(x_1,x_2'))y_2 + \cdots y_ng(k_1(x_n,x_n'))y_n\\
			\text{Since}\ g(k_1(x_i,x_j)) \geq 0\\
			y^TK^{(5)}y &\geq 0 \ \forall \ y \in \mathbf{R}
			\end{align*}
			Thus $k_5$ is a kernel
		}
	\end{homeworkSection}
	
	\begin{homeworkSection}{\homeworkProblemName: ~(d)}
		\problemAnswer{
			$k_6(x,x') = k_1(x,x')k_2(x,x')$
			
			Thus, in terms of our earlier defined matrix notation,
			$K^{(6)} = K^{(1)} \circ K^{(2)}$ where $\circ$ denotes element wise multiplication (also known as the Hadamard product).
			
			
			Since, $k_1$ and $k_2$ are valid kernel function $\exists v_i w_j$ the eigen vectors of matrix $K_1$ and $K_2$ defines such that:
			
			$K^{(1)} = \sum_{i} \lambda_i v_i v_i^T$ and $K^{(2)} = \sum_{j} \mu_j {w_j}{w_j}^T$
			
			Now,\begin{align*}
			K^{(6)} &=  K^{(1)} \circ K^{(2)}\\
			&=  \sum_{i} \lambda_i v_i v_i^T \circ \sum_{j} \mu_j {w_j}{w_j}^T\\
			&= \sum_{i,j} \lambda_i \mu_j  (v_i v_i^T) \circ {w_j}{w_j}^T\\
			&= 	\sum_{i,j} \lambda_i \mu_j  (v_i \circ w_j) ({v_j}\circ {w_j})^T\\
			&\geq 0
			\end{align*}
			Because $(v_i \circ w_j) ({v_j}\circ {w_j})^T  = ||v_i w_j||_2^2\geq 0$ 
			
		}
	\end{homeworkSection}
	\begin{homeworkSection}{\homeworkProblemName: ~(e)}
		\problemAnswer{
			$k_7(x,x') = exp(k_1(x,x'))$
			
			Just like subpart (c), here $g(x) = exp(x)$ (it's not a polynomial, though that does not affect the derivation we came up with in part (c)). So this is immediate from part (c).
		}
	\end{homeworkSection}
\end{homeworkProblem}

\begin{homeworkProblem}[Problem 5]
\begin{homeworkSection}{a}
	\problemAnswer{
		-----$g_1$-------
		MSE:0.463977    Bias Sq:0.108996        Variance:0.000000\\
		-----$g_2$-------
		MSE:0.356683    Bias Sq:0.002941        Variance:0.003295\\
		-----$g_3$-------
		MSE:0.354618    Bias Sq:0.002844        Variance:0.007814\\
		-----$g_4$-------
		MSE:0.004551    Bias Sq:0.000151        Variance:0.003862\\
		-----$g_5$-------
		MSE:0.005546    Bias Sq:0.000151        Variance:0.004782\\
		----$g_6$-------
		MSE:0.006223    Bias Sq:0.000125        Variance:0.005273\\
		
		As the model complexity increase the squared bias decreases  and the variance increases. However for some reason, the variance attributed with $g_3$ is a bit more than the normal trend. I could not think of a possible explanation for this.
	}
		\begin{figure}
			\includegraphics[scale=0.7]{programming/g1-mse}
			\caption{Problem 5.a $g_1$ MSE}
		\end{figure} 
		\begin{figure}
			\includegraphics[scale=0.7]{programming/g2-mse}
			\caption{Problem 5.a $g_2$ MSE}
		\end{figure} 
		\begin{figure}
			\includegraphics[scale=0.7]{programming/g3-mse}
			\caption{Problem 5.a $g_3$ MSE}
		\end{figure} 
		\begin{figure}
			\includegraphics[scale=0.7]{programming/g4-mse}
			\caption{Problem 5.a $g_4$ MSE}
		\end{figure} 
		\begin{figure}
			\includegraphics[scale=0.7]{programming/g5-mse}
			\caption{Problem 5.a $g_5$ MSE}
		\end{figure} 
		\begin{figure}
			\includegraphics[scale=0.7]{programming/g6-mse}
			\caption{Problem 5.a $g_6$ MSE}
		\end{figure} 
		
		
\end{homeworkSection}
\begin{homeworkSection}{}
	content...
\end{homeworkSection}
\end{homeworkProblem}

\end{document}
