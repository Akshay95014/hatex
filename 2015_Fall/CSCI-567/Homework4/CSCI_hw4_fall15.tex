%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Structured General Purpose Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%       PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{tikz}



\usepackage{amsmath}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in 

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ : \hmwkTitle} % Top center header
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%       DOCUMENT STRUCTURE COMMANDS
%       Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}
   
%----------------------------------------------------------------------------------------
%       NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment\ \#4 } % Assignment title
\newcommand{\hmwkDueDate}{Monday,\ November\ 9,\ 2015} % Due date
\newcommand{\hmwkClass}{CSCI-567} % Course/class
\newcommand{\hmwkClassTime}{} % Class/lecture time
\newcommand{\hmwkAuthorName}{Saket Choudhary} % Your name
\newcommand{\hmwkAuthorID}{2170058637} % Teacher/lecturer
%----------------------------------------------------------------------------------------
%       TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassTime}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName} \\
        \textbf{\hmwkAuthorID}
        }
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%       TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

\newpage
\tableofcontents
\newpage

\begin{homeworkProblem}[Problem 1] % Custom section title
\begin{homeworkSection}{\homeworkProblemName: ~(a) Gradient Calculation}
\problemAnswer{
    $$
      L(y_i, \hat{y_i}) = \log(1+\exp(-y_i\hat{y_i}))
    $$
    \begin{align*}
        g_i &= \frac{\partial L(y_i, \hat{y_i})}{\partial \hat{y_i}}\\
        \Aboxed{g_i   &= \frac{-y_i\exp(-y_i\hat{y_i})}{1+\exp(-y_i\hat{y_i})}}
    \end{align*}
}
\end{homeworkSection}

\begin{homeworkSection}{\homeworkProblemName: ~(b)  Weak Learner Section}
\problemAnswer{

\begin{eqnarray*}
	h^* = \min_{h \in H} \big( \min_{\gamma \in R} \sum_{i=1}^n (-g_i-\gamma h(x_i))^2 \big)\\
	\implies 
    \frac{\partial h^* }{\partial \gamma} =0\\
   \implies 2\sum_{i=1}^n (-g_i-\gamma h(x_i))(-h(x_i)) = 0\\
    \Aboxed{\hat{h} = -\frac{\sum_{i=1}^n g_ih(x_i)}{\sum_{i=1}^n h(x_i)^2}}
\end{eqnarray*}

Also check if it is indeed minimum with a second derivative test:
\begin{eqnarray*}
\frac{\partial^2h^* }{\partial \gamma^2} = 2 \sum_{i=1}^n h(x_i)^2 > 0
\end{eqnarray*}

Since the second derivative is positive definite, $\hat{gamma}$ is indeed where the minima occurs.
}
\end{homeworkSection}

\begin{homeworkSection}{\homeworkProblemName: ~(c)  Step Size Selection}
    \problemAnswer{
        $$
	        \alpha^* = \arg \min_{\alpha \in R} \sum_1^n L(y_i, \hat{y_i}+\alpha h^*(x_i))         
        $$
        Newton's approximation:

        $$
        \alpha_1 = \alpha_0 - \frac{f'(\alpha_0)}{f''(\alpha_0)}
        $$
        We start from $\alpha_0=0$
        and hence:
        
		\begin{align*}
		f(\alpha_0) &=   \sum_{i=1}^n \log(1+\exp(-y_i\hat{y_i})) \\
		f'(\alpha_0) & = \sum_{i=1}^n \frac{\partial L}{\partial \alpha }\\
		 &= \sum_{i=1}^n \frac{-y_i h^*(x_i)\exp{(-y_i(\hat{y_i}+\alpha h^*(x_i)))}}{1+\exp{(-y_i(\hat{y_i}+\alpha h^*(x_i)))}}\\
		 &=  -\sum_{i=1}^n \frac{y_ih^*(x_i)\exp(-y_i\hat{y_i})}{1+\exp(-y_i\hat{y_i})}
		\end{align*}  
		      
		And,
		
		\begin{align*}
		f''(\alpha_0) &= \sum_{i=1}^n \frac{\partial^2 L}{\partial \alpha^2}\\
		&= \sum_{i=1}^n \frac{\big\{\big(1+\exp{(-y_i(\hat{y_i}+\alpha h^*(x_i)))}\big)(y_ih^*(x_i))^2  +y_ih^*(x_i) \big\} \exp{(-y_i(\hat{y_i}+\alpha h^*(x_i)))}}{(1+\exp{(-y_i(\hat{y_i}+\alpha h^*(x_i)))})^2}\\
		&= \sum_{i=1}^n \frac{\big\{\big(1+\exp{(-y_i\hat{y_i})}\big)(y_ih^*(x_i))^2  +y_ih^*(x_i) \big\} \exp{(-y_i\hat{y_i})}}{(1+\exp{(-y_i\hat{y_i})})^2}\\
		\end{align*}
		      Thus,
		      $$
		      %\alpha_1 = \frac{\sum_{i=1}^n(\log(1+\exp(-y_i\hat{y_i})))}{\sum_{i=1}^n \frac{-y_ih^*(x_i)\exp(-y_i\hat{y_i})}{1+\exp(-y_i\hat{y_i})}}
		      \alpha_1 = \frac{\sum_{i=1}^n \frac{y_ih^*(x_i)\exp(-y_i\hat{y_i})}{1+\exp(-y_i\hat{y_i})}}{\sum_{i=1}^n \frac{\big\{\big(1+\exp{(-y_i\hat{y_i})}\big)(y_ih^*(x_i))^2  +y_ih^*(x_i) \big\} \exp{(-y_i\hat{y_i})}}{(1+\exp{(-y_i\hat{y_i})})^2}}
		      $$
    }

\end{homeworkSection}


\end{homeworkProblem}

\begin{homeworkProblem}[Problem 2]
    \begin{homeworkSection}{\homeworkProblemName: ~(a)}
        \problemAnswer{
            Primal form:
                \begin{align*} 
                    \min_w  ||w||^2\\
                    \text{such that } |y_i-(w^Tx_i+b)| \leq \epsilon
                \end{align*}
        }
    \end{homeworkSection}
    
    \begin{homeworkSection}{\homeworkProblemName: ~(b)}
        \problemAnswer{ 
            \begin{align*}
            \min_{_w,\epsilon_i}  \frac{1} {2}||w||^2 +  C\sum_i \epsilon_i\\
            \text{such that } (w^Tx_i+b) - y_i \leq n_i + \epsilon_i \text{ (positive deviation)}\\
             \text{ and } y_i-(w^Tx_i+b) \leq {p_i}+ \epsilon_i \text{ (negative deviation)}\\
	         n_i \geq 0\\
	         p_i \geq 0
            \end{align*}
            
            Also,
            the slackness loss needs further constraints:
            \begin{align*}
            n_i &= \begin{cases}
            0 & |n_i| < \epsilon_i,\\
            |n_i|-\epsilon & otherwise
            \end{cases}\\
            p_i &= \begin{cases}
            0 & |p_i| < \epsilon_i,\\
            |p_i|-\epsilon & otherwise
            \end{cases}
            \end{align*}
            \begin{center}
            	

            \begin{tikzpicture}
            \draw[->] (-3,0) -- (3.2,0) node[right] {$x$};
            \draw[->] (0,-3) -- (0,3.2) node[above] {$y$};
            \draw[scale=0.5,domain=1:3,smooth,variable=\x,blue] plot ({\x},{\x-1});
           \draw[scale=0.5,domain=-3:-1,smooth,variable=\x,blue] plot ({\x},{-\x-1});
           \filldraw[black] (-0.5,0) circle (2pt) node[anchor=south]   {$-\epsilon_i$};
            \filldraw[black] (0.5,0) circle (2pt) node[anchor=south]   {$\epsilon_i$};
            \end{tikzpicture}
            \end{center}
            
            So essentially $n_i,p_i$ are non zero, only above the two blue lines
        }
    \end{homeworkSection}
    \begin{homeworkSection}{\homeworkProblemName: ~(c)}
        \problemAnswer{
\begin{align*}
L  &= \frac{1}{2}||w||^2 + C\sum_i (p_i+n_i)\\
 &- \sum_i(\eta_ip_i+\eta_i'n_i)\\ 
 &-\sum_i \alpha_i(\epsilon+ p_i - (y_i-(w^Tx_i+b)))\\
 &- \sum_i \beta_i (\epsilon+ n_i + (y_i-(w^Tx_i+b)))
\end{align*}

{\centering{\textbf{Conditions	}:}}
\begin{align*}
\alpha_i &\geq 0\\
\beta_i &\geq 0\\
\eta_i &\geq 0\\
\eta_i^* &\geq 0
\end{align*}


Dual Form:
\begin{align*}
\Delta_w L &=0 \\
&= w - \sum_i \alpha_i x_i + \sum_i \beta_i x_i = 0\\
\Delta_b L &=0\\
&= \sum_i \alpha_i - \sum_i \beta_i = 0\\
\Delta_{p_i} L &= 0\\
&= C-\sum \eta_i -\sum_i \alpha_i   =0\\
\Delta_{n_i} L &= 0\\
&= C-\sum \eta'_i -\sum_i \beta_i   =0\\
\end{align*}
}
\problemAnswer{

Thus, $w$ is given by:
$$
w = \sum_i \alpha_i x_i - \sum_i \beta_i x_i
$$

depends only on the support vectors.


This reduces the optimisation to:

\begin{align*}
\max f &= \frac{1}{2} \sum_{i,j} (\alpha_i-\beta_i)x_i^Tx_j(\alpha_j-\beta_j) + p_i(C-\sum_i \eta_i - \sum_i \alpha_i)\\ 
&+ n_i(C+\sum_i \eta'_i -\sum_i \beta_i)\\
&+ \epsilon(-\sum_i \alpha_i - \sum_i \beta_i)\\ 
&+ \sum_i y_i(\alpha_i-\beta_i)\\
&= \frac{1}{2} \sum_{i,j} (\alpha_i-\beta_i)x_i^Tx_j(\alpha_j-\beta_j)  - \epsilon (\sum_i(\alpha_i+\beta_i))\\
&+ \sum_i y_i(\alpha_i-\beta_i)\\
\text{such that} \sum_i(\alpha_i-\beta_i) &= 0 \\
\text{and } \alpha_i,\beta_i &\in [0,C]
\end{align*}

        }
    \end{homeworkSection}
    \begin{homeworkSection}{\homeworkProblemName: ~(d)}
\problemAnswer{
	Using Kernel transformation:
	$$
	w = \sum_i(\alpha_i-\beta_i)\phi(x_i)
	$$	
	this happens because $x_i'x_j$ gets mapped onto by an equivalent kernel function $k(x_i,x_j)=\phi^T(x_i)\phi(x_j)$
	and the objective function is:
	$$
	\max_f = \frac{1}{2} \sum_{i,j} (\alpha_i-\beta_i)k(x_i, x_j)(\alpha_j-\beta_j)  - \epsilon (\sum_i(\alpha_i+\beta_i))
	$$
	}
	
    \end{homeworkSection}
\end{homeworkProblem}

\begin{homeworkProblem}[Problem 3.3]
	\begin{homeworkSection}{\homeworkProblemName: ~(a)}
		\problemAnswer{
			\begin{center}
			\begin{tabular}{|c|c|c|c|c|c|c|}
				\hline
C	& Tr. Dataset 1(t) & Tr. Dataset 2(t) & Training Dataset 3(t)	& CV & Avg. t \\
\hline
$4^{-6}$=0.000244 & 0.606156 & 0.400791 & 0.346078 & 0.578976 & 0.451 \\
\hline
$4^{-5}$=0.000977 & 0.379495 & 0.477559 & 0.498054 & 0.907001 & 0.451\\
\hline
$4^{-4}$=0.003906 & 0.529940 & 0.495841 & 0.534946 & 0.926001 & 0.520\\
\hline
$4^{-3}$=0.015625 & 0.516236 & 0.577324 & 0.561874 & 0.935501 & 0.551\\
\hline
$4^{-2}$=0.062500 & 0.512287 & 0.529517 & 0.554510 & 0.945006 & 0.532\\
\hline
$4^{-1}$=0.250000 & 0.630195 & 0.663459 & 0.657651 & 0.943010 & 0.650\\
\hline 
$4^{0}$=1.000000 & 0.746649 & 0.601710 & 0.563063 & 0.939003 & 0.637\\
\hline
$4^{1}$=4.000000 & 0.633370 & 0.572011 & 0.595552 & 0.942501 & 0.600\\
\hline
$4^2$=16.000000 & 0.674677 & 0.681010 & 0.698041 & 0.943503 & 0.684\\
\hline
			\end{tabular}
				
			\end{center}
			
			As seen from the table. the time seems to increase with $C$ and the $CV$ increases too.
			C determines the tradeoff between objective function complexity and the overall loss. When $C$ is small, there are chances of overfitting, this is evident from low $CV$ values for lower $C$(because the generalisation error is high)
			
			In terms of time complexity, if 
			
			The larger the value of $C$, the more is the penalisation and hence smaller the $\epsilon_i$ would be
			

			}
	\end{homeworkSection}
	\begin{homeworkSection}{\homeworkProblemName: ~(b)}
		\problemAnswer{
Based on lowest cross validation error. $C=4^2$ 
			}
	\end{homeworkSection}
	
	\begin{homeworkSection}{\homeworkProblemName: ~(c)}
		\problemAnswer{
		With $C=16$, test accuracy = $0.943500$
		}
	\end{homeworkSection}
	
\end{homeworkProblem}

\begin{homeworkProblem}[Problem 3.4]
	\begin{homeworkSection}{\homeworkProblemName: ~(a)}
		\problemAnswer{
			Platform Used: $Ubuntu\ 12.04,\ x86\_64$\\
			`libsvm` gives $0.9655$ as its accuracy which is pretty close to  0.943 that my code gives.
			\begin{center}
				\begin{tabular}{|c|c|c|c}
					\hline
					C & Training Time & CV\\
					\hline
					$4^{-6}$ & 0.829624 & 0.5575 \\
					\hline
					$4^{-5}$ & 0.827846 & 0.5575\\
					\hline
					$4^{-4}$ & 0.831586 & 0.5575\\
					\hline
					$4^{-3}$ & 0.831599 & 0.7295\\
					\hline
					$4^{-2}$ & 0.649459 & 0.9195\\
					\hline
					$4^{-1}$ & 0.425499 & 0.934\\
					\hline 
					$4^{0}$ & 0.281219 & 0.949\\
					\hline
					$4^{1}$ & 0.210071 & 0.955\\
					\hline
					$4^2$ & 0.201989 & 0.9655\\
					\hline
				\end{tabular}
						\end{center}
			}
	\end{homeworkSection}
	
		\begin{homeworkSection}{\homeworkProblemName: ~(b)}
			\problemAnswer{
				`libsvm` gives $0.9455$ as its accuracy which is pretty close to 
		
			}
		\end{homeworkSection}
		
		
\end{homeworkProblem}


\begin{homeworkProblem}[Problem 3.5]
	\begin{homeworkSection}{\homeworkProblemName: ~(a)}
		\problemAnswer{
			\begin{center}
						\begin{tabular}{|c|c|c|}
				\hline
				C & Training Time & CV\\
				\hline 
				0.015625 & 0.675134 & 0.7010\\
				\hline
				0.062500 & 0.680862 & 0.5575\\
				\hline
				0.250000 & 0.682373 & 0.5575\\
				\hline
				1.000000 & 0.532627 & 0.9175\\
				\hline
				4.000000 & 0.608079 & 0.8655\\
				\hline
				16.000000 & 0.678589 & 0.7690\\
				\hline
				64.000000 & 0.347019 & 0.9280\\
				\hline
				256.000000 & 0.432110 & 0.9255\\
				\hline
				1024.000000 & 0.523863 & 0.9180\\
				\hline
				4096.000000 & 0.239251 & 0.9385\\
				\hline
				16384.000000 & 0.284755 & 0.9470\\
				\hline
				
			\end{tabular}
			
			\end{center}
			
		}
	\end{homeworkSection}
	
	\begin{homeworkSection}{\homeworkProblemName: ~(b)}
		\problemAnswer{
				\begin{center}
					\begin{tabular}{|c|c|c|}
						\hline
						C & Training Time & CV\\
						\hline 
		0.015625 & 0.806863 & 0.5575\\
		\hline
		0.062500 & 0.804094 & 0.5575\\
		\hline
		0.250000 & 0.804537 & 0.5575\\
		\hline
		1.000000 & 0.807285 & 0.5575\\
		\hline
		4.000000 & 0.810119 & 0.6475\\
		\hline
		16.000000 & 0.815702 & 0.7375\\
		\hline
		64.000000 & 0.828215 & 0.55750\\
		\hline
		256.000000 & 0.803329 & 0.5575\\
		\hline
		1024.000000 & 0.803978 & 0.5575\\
		\hline
		4096.000000 & 0.806347 & 0.5575\\
		\hline
		16384.000000 & 0.816474 & 0.8335\\
		\hline
			\end{tabular}
			
		\end{center}
		
		}
	\end{homeworkSection}
	\problemAnswer{
		Based on the RBF, polykernel:
		
		Polynomal Kernel train accuracy: 96.600000\\
		RBF Kernel train accuracy: 96.900000\\
		Better kernel: RBF\\
		Polynomial Kernel optimal C: 64.000000\\
		Polynomial Kernel optimal degree: 2.000000\\
		Polynomal Kernel test accuracy: 95.150000\\
		RBF Kernel optimal g: 0.062500\\
		RBF Kernel optimal c: 16.000000\\
		RBF Kernel test accuracy: 96.500000\\
		
		}
	
\end{homeworkProblem}
\end{document}
